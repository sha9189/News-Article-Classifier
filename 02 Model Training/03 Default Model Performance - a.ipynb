{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "import pickle\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import random\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training   \n",
    "\n",
    "Now that we have our feature vectors built, we'll try several machine learning classification models in order to find which one performs best on our data. We will try with the following models:\n",
    "\n",
    "1. Baseline Classifier\n",
    "1. Random Forest   \n",
    "2. Support Vector Machine   \n",
    "3. K Nearest Neighbors   \n",
    "4. Multinomial Na√Øve Bayes   \n",
    "5. Multinomial Logistic Regression   \n",
    "6. Gradient Boosting   \n",
    "\n",
    "We will use hyperparameter tuning to obtain to set of parameters for the best performance.   \n",
    "\n",
    "We will use **accuracy** as a performance metric for our models.\n",
    "\n",
    "Also, when we get news articles that don't belong to any of that categories (for example, weather or terrorism news articles), we will surely get a wrong prediction. For this reason we will take into account the conditional probability of belonging to every class and set a lower threshold (i.e. if the 5 conditional probabilities are lower than 45% then the prediction will be 'other'). This probability vector can be obtained in a simple way in some models, but not in other ones. For this reason we will take this into consideration when choosing the model to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the performance of default models so that we can eliminate any model that is doing very poorly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"D:\\\\Projects\\\\News Article classifier\\\\01 Dataset Creation\\\\Pickles\\\\\"\n",
    "\n",
    "filename = \"features_test.pickle\"\n",
    "with open(folder_path + filename, \"rb\") as file:\n",
    "    features_test = pickle.load(file)\n",
    "    \n",
    "filename = \"features_train.pickle\"\n",
    "with open(folder_path + filename, \"rb\") as file:\n",
    "    features_train = pickle.load(file)\n",
    "    \n",
    "filename = \"labels_test.pickle\"\n",
    "with open(folder_path + filename, \"rb\") as file:\n",
    "    labels_test = pickle.load(file)\n",
    "\n",
    "filename = \"labels_train.pickle\"\n",
    "with open(folder_path + filename, \"rb\") as file:\n",
    "    labels_train = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.236669\n",
       "3    0.223487\n",
       "2    0.190533\n",
       "4    0.182145\n",
       "1    0.167166\n",
       "Name: Category_codes, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observataion : A model that always predicts y = 3 has a 23.4% chances of being accurate. Let this be the baseline classifier with accuracy = 23.4%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "accu_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "svm_1 = svm.SVC(random_state=36)\n",
    "svm_1.fit(features_train, labels_train)\n",
    "ypred = svm_1.predict(features_test)\n",
    "accu_dict[\"SVM\"] = accuracy_score(ypred, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "rf_1 = RandomForestClassifier(random_state=23)\n",
    "rf_1.fit(features_train, labels_train)\n",
    "ypred = rf_1.predict(features_test)\n",
    "accuracy_score(ypred, labels_test)\n",
    "accu_dict[\"Random Forest\"] = accuracy_score(ypred, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multinomial NB\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(features_train, labels_train)\n",
    "ypred = mnb.predict(features_test)\n",
    "accuracy_score(ypred, labels_test)\n",
    "accu_dict[\"Multinomial NB\"] = accuracy_score(ypred, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multinomial Logistic regression\n",
    "model1 = LogisticRegression(random_state=43)\n",
    "model1.fit(features_train, labels_train)\n",
    "ypred = model1.predict(features_test)\n",
    "accuracy_score(ypred, labels_test)\n",
    "accu_dict[\"Multinomial Logistic regression\"] = accuracy_score(ypred, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "knn1 = KNeighborsClassifier()\n",
    "knn1.fit(features_train, labels_train)\n",
    "y_pred = knn1.predict(features_test)\n",
    "accuracy_score(y_pred, labels_test)\n",
    "accu_dict[\"KNN\"] = accuracy_score(ypred, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Booosting Classifier\n",
    "gbc_1 = GradientBoostingClassifier(random_state=29)\n",
    "gbc_1.fit(features_train, labels_train)\n",
    "ypred = gbc_1.predict(features_test)\n",
    "accuracy_score(ypred, labels_test)\n",
    "accu_dict[\"Gradient Booosting Classifier\"] = accuracy_score(ypred, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Random Forest', 0.9443447037701975),\n",
       " ('Gradient Booosting Classifier', 0.9443447037701975),\n",
       " ('SVM', 0.940754039497307),\n",
       " ('Multinomial NB', 0.9389587073608617),\n",
       " ('Multinomial Logistic regression', 0.9389587073608617),\n",
       " ('KNN', 0.9389587073608617)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(accu_dict.items(), key = lambda x: x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation : We see that all models have a high accuracy. Hence we will not elimiinate any model and perform hyperparameter tuning in all of the models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
